{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67daf4cd-accd-4a14-b4f0-c6e01ef938af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка исходные данные/MOCK_DATA (1).csv → после парсинга/parsed_mock1.csv\n",
      "Обработка исходные данные/MOCK_DATA (2).csv → после парсинга/parsed_mock2.csv\n",
      "Обработка исходные данные/MOCK_DATA (3).csv → после парсинга/parsed_mock3.csv\n",
      "Обработка исходные данные/MOCK_DATA (4).csv → после парсинга/parsed_mock4.csv\n",
      "Обработка исходные данные/MOCK_DATA (5).csv → после парсинга/parsed_mock5.csv\n",
      "Обработка исходные данные/MOCK_DATA (6).csv → после парсинга/parsed_mock6.csv\n",
      "Обработка исходные данные/MOCK_DATA (7).csv → после парсинга/parsed_mock7.csv\n",
      "Обработка исходные данные/MOCK_DATA (8).csv → после парсинга/parsed_mock8.csv\n",
      "Обработка исходные данные/MOCK_DATA (9).csv → после парсинга/parsed_mock9.csv\n",
      "Обработка исходные данные/MOCK_DATA.csv → после парсинга/parsed_mock10.csv\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_csv_record(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Очищает строку CSV:\n",
    "    - Убирает лишние пробелы\n",
    "    - Убирает пробелы вокруг запятых и знака равенства\n",
    "    \"\"\"\n",
    "    line = line.rstrip('\\r\\n')\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)\n",
    "    line = re.sub(r\"\\s*,\\s*\", \",\", line)\n",
    "    line = re.sub(r\"\\s*=\\s*\", \"=\", line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def process_file(input_path: Path, output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Читает input_path, соединяет разбитые записи и записывает очищенный CSV в output_path.\n",
    "    Каждая запись на отдельной строке, записи начинаются с числа.\n",
    "    \"\"\"\n",
    "    new_record = re.compile(r\"^\\d+,\")\n",
    "    output_lines = []\n",
    "    buffer = []\n",
    "\n",
    "    with input_path.open('r', encoding='utf-8') as fin:\n",
    "        for raw in fin:\n",
    "            cleaned = clean_csv_record(raw)\n",
    "            if new_record.match(cleaned):\n",
    "                if buffer:\n",
    "                    # добавляем накопленную запись\n",
    "                    output_lines.append(''.join(buffer))\n",
    "                buffer = [cleaned]\n",
    "            else:\n",
    "                buffer.append(cleaned)\n",
    "\n",
    "    if buffer:\n",
    "        output_lines.append(''.join(buffer))\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_path.open('w', encoding='utf-8', newline='\\n') as fout:\n",
    "        fout.write(\"\\n\".join(output_lines))\n",
    "\n",
    "\n",
    "\n",
    "raw_dir = Path('./исходные данные')\n",
    "out_dir = Path('./после парсинга')\n",
    "pattern = 'MOCK_DATA*.csv'\n",
    "files = sorted(raw_dir.glob(pattern))\n",
    "if not files:\n",
    "    print(f\"Нет файлов по шаблону {pattern} в {raw_dir}\")\n",
    "for inp in files:\n",
    "    out_name = f\"parsed_mock{files.index(inp)+1}.csv\"\n",
    "    out_path = out_dir / out_name\n",
    "    print(f\"Обработка {inp} → {out_path}\")\n",
    "    process_file(inp, out_path)\n",
    "print(\"Готово!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60b9f93-c625-4faa-8009-4c4da9ca7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b105173-50d1-408d-a1a0-2014daf5d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDL выполнен\n",
      "Загрузка ././после парсинга/parsed_mock1.csv\n",
      "Загрузка ././после парсинга/parsed_mock10.csv\n",
      "Загрузка ././после парсинга/parsed_mock2.csv\n",
      "Загрузка ././после парсинга/parsed_mock3.csv\n",
      "Загрузка ././после парсинга/parsed_mock4.csv\n",
      "Загрузка ././после парсинга/parsed_mock5.csv\n",
      "Загрузка ././после парсинга/parsed_mock6.csv\n",
      "Загрузка ././после парсинга/parsed_mock7.csv\n",
      "Загрузка ././после парсинга/parsed_mock8.csv\n",
      "Загрузка ././после парсинга/parsed_mock9.csv\n",
      "CSV загружены в mock_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "\n",
    "DB_PARAMS = {\n",
    "    \"host\":     os.getenv(\"DB_HOST\", \"postgres\"),\n",
    "    \"port\":     os.getenv(\"DB_PORT\", \"5432\"),\n",
    "    \"dbname\":   os.getenv(\"DB_NAME\", \"spark_db\"),\n",
    "    \"user\":     os.getenv(\"DB_USER\", \"spark_user\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\", \"spark_password\"),\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "cur = conn.cursor()\n",
    "\n",
    "CSV_DIR = os.getenv(\"CSV_DIR\", \".\")\n",
    "WORK_DIR = os.getenv(\"WORK_DIR\", \".\")\n",
    "\n",
    "def load_file_contents(path):\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp1251\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"Не удалось прочитать файл {path} в поддерживаемой кодировке\")\n",
    "\n",
    "def init_db():\n",
    "    ddl_path = os.path.join(WORK_DIR, \"init.sql\")\n",
    "    ddl_sql = load_file_contents(ddl_path)\n",
    "    cur.execute(ddl_sql)\n",
    "    conn.commit()\n",
    "    print(\"DDL выполнен\")\n",
    "\n",
    "def load_data():\n",
    "    pattern = os.path.join(CSV_DIR, \"./после парсинга/parsed_mock*.csv\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    for path in files:\n",
    "        print(f\"Загрузка {path}\")\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cur.copy_expert(\"COPY mock_data FROM STDIN WITH CSV HEADER\", f)\n",
    "    conn.commit()\n",
    "    print(\"CSV загружены в mock_data\")\n",
    "\n",
    "\n",
    "init_db()\n",
    "load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c130fca1-33d6-425a-a524-f13bbb6582bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Будет вставлено 0 новых записей в dim_countries\n",
      "Будет вставлено 0 новых записей в dim_cities\n",
      "Будет вставлено 0 новых записей в dim_dates\n",
      "Будет вставлено 0 новых записей в dim_pet_types\n",
      "Будет вставлено 0 новых записей в dim_pet_breeds\n",
      "Будет вставлено 0 новых записей в dim_pet_categories\n",
      "Будет вставлено 0 новых записей в dim_pets\n",
      "Будет вставлено 10000 новых записей в dim_suppliers\n",
      "Будет вставлено 3 новых записей в dim_product_categories\n",
      "Будет вставлено 19 новых записей в dim_product_colors\n",
      "Будет вставлено 3 новых записей в dim_product_sizes\n",
      "Будет вставлено 383 новых записей в dim_product_brands\n",
      "Будет вставлено 11 новых записей в dim_product_materials\n",
      "Будет вставлено 10000 новых записей в dim_products\n",
      "Будет вставлено 10000 новых записей в dim_customers\n",
      "Будет вставлено 10000 новых записей в dim_sellers\n",
      "Будет вставлено 9688 новых записей в dim_stores\n",
      "Будет вставлено 10000 фактов в fact_sales\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "ETL-процесс: загрузка данных из PostgreSQL, формирование звёздной схемы и сохранение в ClickHouse.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, year, quarter, month,\n",
    "    dayofmonth, date_format\n",
    ")\n",
    "\n",
    "\n",
    "def initialize_postgres_session():\n",
    "    session = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Postgres_ETL_StarSchema\")\n",
    "        .config(\"spark.jars\", \"postgresql-42.6.0.jar\")\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return session\n",
    "\n",
    "\n",
    "def load_raw_data(spark_sess, url, properties):\n",
    "    df = (\n",
    "        spark_sess.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", url)\n",
    "        .option(\"dbtable\", \"mock_data\")\n",
    "        .options(**properties)\n",
    "        .load()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def refresh_countries_dimension(raw_df, url, props):\n",
    "    customers = raw_df.select(col(\"customer_country\").alias(\"country_name\"))\n",
    "    sellers = raw_df.select(col(\"seller_country\").alias(\"country_name\"))\n",
    "    stores = raw_df.select(col(\"store_country\").alias(\"country_name\"))\n",
    "    suppliers = raw_df.select(col(\"supplier_country\").alias(\"country_name\"))\n",
    "\n",
    "    extracted = (\n",
    "        customers.union(sellers)\n",
    "                 .union(stores)\n",
    "                 .union(suppliers)\n",
    "                 .where(col(\"country_name\").isNotNull())\n",
    "                 .distinct()\n",
    "    )\n",
    "\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_countries\", properties=props\n",
    "    )\n",
    "\n",
    "    to_add = extracted.join(existing, [\"country_name\"], \"left_anti\") \\\n",
    "                      .dropDuplicates([\"country_name\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_countries\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_countries\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_cities_dimension(raw_df, url, props):\n",
    "    stores = raw_df.select(col(\"store_city\").alias(\"city_name\"))\n",
    "    suppliers = raw_df.select(col(\"supplier_city\").alias(\"city_name\"))\n",
    "\n",
    "    extracted = (\n",
    "        stores.union(suppliers)\n",
    "              .where(col(\"city_name\").isNotNull())\n",
    "              .distinct()\n",
    "    )\n",
    "\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_cities\", properties=props\n",
    "    )\n",
    "\n",
    "    to_add = extracted.join(existing, [\"city_name\"], \"left_anti\") \\\n",
    "                      .dropDuplicates([\"city_name\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_cities\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_cities\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_dates_dimension(raw_df, url, props):\n",
    "    sales = raw_df.select(to_date(\"sale_date\", \"MM/dd/yyyy\").alias(\"full_date\"))\n",
    "    releases = raw_df.select(to_date(\"product_release_date\", \"MM/dd/yyyy\").alias(\"full_date\"))\n",
    "    expiries = raw_df.select(to_date(\"product_expiry_date\", \"MM/dd/yyyy\").alias(\"full_date\"))\n",
    "\n",
    "    extracted = (\n",
    "        sales.union(releases)\n",
    "             .union(expiries)\n",
    "             .where(col(\"full_date\").isNotNull())\n",
    "             .distinct()\n",
    "             .withColumn(\"year\", year(\"full_date\"))\n",
    "             .withColumn(\"quarter\", quarter(\"full_date\"))\n",
    "             .withColumn(\"month\", month(\"full_date\"))\n",
    "             .withColumn(\"day\", dayofmonth(\"full_date\"))\n",
    "             .withColumn(\"weekday\", date_format(\"full_date\", \"EEEE\"))\n",
    "    )\n",
    "\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_dates\", properties=props\n",
    "    )\n",
    "\n",
    "    to_add = extracted.join(existing, [\"full_date\"], \"left_anti\") \\\n",
    "                      .dropDuplicates([\"full_date\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_dates\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_dates\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_simple_lookup(raw_df, url, props):\n",
    "    mapping = [\n",
    "        (\"customer_pet_type\", \"dim_pet_types\", \"pet_type_name\"),\n",
    "        (\"customer_pet_breed\", \"dim_pet_breeds\", \"pet_breed_name\"),\n",
    "        (\"pet_category\", \"dim_pet_categories\", \"pet_category_name\"),\n",
    "    ]\n",
    "\n",
    "    for src_col, table_name, alias in mapping:\n",
    "        extracted = (\n",
    "            raw_df.select(col(src_col).alias(alias))\n",
    "                  .where(col(alias).isNotNull())\n",
    "                  .distinct()\n",
    "        )\n",
    "\n",
    "        existing = raw_df.sparkSession.read.jdbc(\n",
    "            url=url, table=table_name, properties=props\n",
    "        )\n",
    "\n",
    "        to_add = extracted.join(existing, [alias], \"left_anti\") \\\n",
    "                          .dropDuplicates([alias])\n",
    "        add_count = to_add.count()\n",
    "        print(f\"Будет вставлено {add_count} новых записей в {table_name}\")\n",
    "\n",
    "        if add_count > 0:\n",
    "            to_add.write.jdbc(url=url, table=table_name, mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_pets_dimension(raw_df, url, props):\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_pets\", properties=props\n",
    "    ).select(\"pet_name\", \"pet_type_id\").dropDuplicates([\"pet_name\", \"pet_type_id\"])\n",
    "\n",
    "    base = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "            col(\"customer_pet_type\").alias(\"pet_type_name\"),\n",
    "            col(\"customer_pet_breed\").alias(\"pet_breed_name\"),\n",
    "            col(\"pet_category\").alias(\"pet_category_name\")\n",
    "        )\n",
    "        .where(col(\"pet_name\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    pet_types = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_pet_types\", properties=props\n",
    "    )\n",
    "    pet_breeds = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_pet_breeds\", properties=props\n",
    "    )\n",
    "    pet_cats = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_pet_categories\", properties=props\n",
    "    )\n",
    "\n",
    "    prepared = (\n",
    "        base\n",
    "        .join(pet_types,  base.pet_type_name  == pet_types.pet_type_name,   \"left\")\n",
    "        .join(pet_breeds, base.pet_breed_name == pet_breeds.pet_breed_name, \"left\")\n",
    "        .join(pet_cats,   base.pet_category_name == pet_cats.pet_category_name, \"left\")\n",
    "        .select(\n",
    "            col(\"pet_name\"),\n",
    "            col(\"pet_type_id\"),\n",
    "            col(\"pet_breed_id\"),\n",
    "            col(\"pet_category_id\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    to_add = prepared.join(existing, [\"pet_name\", \"pet_type_id\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"pet_name\", \"pet_type_id\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_pets\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_pets\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_suppliers_dimension(raw_df, url, props):\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_suppliers\", properties=props\n",
    "    ).select(\"supplier_email\").dropDuplicates([\"supplier_email\"])\n",
    "\n",
    "    base = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"supplier_name\"),\n",
    "            col(\"supplier_contact\").alias(\"contact_person\"),\n",
    "            col(\"supplier_email\"),\n",
    "            col(\"supplier_phone\"),\n",
    "            col(\"supplier_address\"),\n",
    "            col(\"supplier_city\").alias(\"city_name\"),\n",
    "            col(\"supplier_country\").alias(\"country_name\")\n",
    "        )\n",
    "        .where(col(\"supplier_email\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    cities = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_cities\", properties=props\n",
    "    )\n",
    "    countries = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_countries\", properties=props\n",
    "    )\n",
    "\n",
    "    prepared = (\n",
    "        base\n",
    "        .join(cities,   base.city_name    == cities.city_name,   \"left\")\n",
    "        .join(countries, base.country_name == countries.country_name, \"left\")\n",
    "        .select(\n",
    "            col(\"supplier_name\"),\n",
    "            col(\"contact_person\"),\n",
    "            col(\"supplier_email\"),\n",
    "            col(\"supplier_phone\"),\n",
    "            col(\"supplier_address\"),\n",
    "            col(\"city_id\"),\n",
    "            col(\"country_id\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_add = prepared.join(existing, [\"supplier_email\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"supplier_email\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_suppliers\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_suppliers\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_product_dimensions(raw_df, url, props):\n",
    "    lookup_info = [\n",
    "        (\"product_category\", \"category_name\", \"dim_product_categories\"),\n",
    "        (\"product_color\",    \"color_name\",    \"dim_product_colors\"),\n",
    "        (\"product_size\",     \"size_name\",     \"dim_product_sizes\"),\n",
    "        (\"product_brand\",    \"brand_name\",    \"dim_product_brands\"),\n",
    "        (\"product_material\", \"material_name\", \"dim_product_materials\"),\n",
    "    ]\n",
    "\n",
    "    for src_col, alias, table_name in lookup_info:\n",
    "        extracted = (\n",
    "            raw_df.select(col(src_col).alias(alias))\n",
    "                  .where(col(alias).isNotNull())\n",
    "                  .distinct()\n",
    "        )\n",
    "\n",
    "        existing = raw_df.sparkSession.read.jdbc(\n",
    "            url=url, table=table_name, properties=props\n",
    "        )\n",
    "\n",
    "        to_add = extracted.join(existing, [alias], \"left_anti\") \\\n",
    "                          .dropDuplicates([alias])\n",
    "        add_count = to_add.count()\n",
    "        print(f\"Будет вставлено {add_count} новых записей в {table_name}\")\n",
    "\n",
    "        if add_count > 0:\n",
    "            to_add.write.jdbc(url=url, table=table_name, mode=\"append\", properties=props)\n",
    "\n",
    "    prod_cats = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_product_categories\", properties=props\n",
    "    )\n",
    "    colors = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_product_colors\", properties=props\n",
    "    )\n",
    "    sizes = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_product_sizes\", properties=props\n",
    "    )\n",
    "    brands = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_product_brands\", properties=props\n",
    "    )\n",
    "    materials = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_product_materials\", properties=props\n",
    "    )\n",
    "    suppliers = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_suppliers\", properties=props\n",
    "    )\n",
    "    dates_dim = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_dates\", properties=props\n",
    "    )\n",
    "\n",
    "    dates_release = dates_dim.select(\n",
    "        col(\"date_id\").alias(\"release_date_id\"),\n",
    "        col(\"full_date\").alias(\"release_full_date\")\n",
    "    )\n",
    "    dates_expiry = dates_dim.select(\n",
    "        col(\"date_id\").alias(\"expiry_date_id\"),\n",
    "        col(\"full_date\").alias(\"expiry_full_date\")\n",
    "    )\n",
    "\n",
    "    raw_products = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"product_name\"),\n",
    "            col(\"supplier_email\"),\n",
    "            col(\"product_category\").alias(\"category_name\"),\n",
    "            col(\"product_price\").alias(\"price\"),\n",
    "            col(\"product_weight\").alias(\"weight\"),\n",
    "            col(\"product_color\").alias(\"color_name\"),\n",
    "            col(\"product_size\").alias(\"size_name\"),\n",
    "            col(\"product_brand\").alias(\"brand_name\"),\n",
    "            col(\"product_material\").alias(\"material_name\"),\n",
    "            col(\"product_description\").alias(\"description\"),\n",
    "            col(\"product_rating\").alias(\"rating\"),\n",
    "            col(\"product_reviews\").alias(\"reviews\"),\n",
    "            to_date(\"product_release_date\", \"MM/dd/yyyy\").alias(\"release_full_date\"),\n",
    "            to_date(\"product_expiry_date\", \"MM/dd/yyyy\").alias(\"expiry_full_date\"),\n",
    "        )\n",
    "        .where(col(\"product_name\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    enriched = (\n",
    "        raw_products\n",
    "        .join(prod_cats, raw_products.category_name == prod_cats.category_name, \"left\")\n",
    "        .join(colors,    raw_products.color_name    == colors.color_name,    \"left\")\n",
    "        .join(sizes,     raw_products.size_name     == sizes.size_name,     \"left\")\n",
    "        .join(brands,    raw_products.brand_name    == brands.brand_name,    \"left\")\n",
    "        .join(materials, raw_products.material_name == materials.material_name,\"left\")\n",
    "        .join(suppliers, raw_products.supplier_email == suppliers.supplier_email,\"left\")\n",
    "        .join(dates_release, raw_products.release_full_date == dates_release.release_full_date, \"left\")\n",
    "        .join(dates_expiry,  raw_products.expiry_full_date  == dates_expiry.expiry_full_date,  \"left\")\n",
    "        .select(\n",
    "            col(\"product_name\"),\n",
    "            col(\"supplier_id\"),\n",
    "            col(\"category_id\"),\n",
    "            col(\"price\"),\n",
    "            col(\"weight\"),\n",
    "            col(\"color_id\"),\n",
    "            col(\"size_id\"),\n",
    "            col(\"brand_id\"),\n",
    "            col(\"material_id\"),\n",
    "            col(\"description\"),\n",
    "            col(\"rating\"),\n",
    "            col(\"reviews\"),\n",
    "            col(\"release_date_id\"),\n",
    "            col(\"expiry_date_id\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_products\", properties=props\n",
    "    ).select(\"product_name\", \"supplier_id\").dropDuplicates([\"product_name\", \"supplier_id\"])\n",
    "\n",
    "    to_add = enriched.join(existing, [\"product_name\", \"supplier_id\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"product_name\", \"supplier_id\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_products\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_products\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_customers_dimension(raw_df, url, props):\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_customers\", properties=props\n",
    "    ).select(\"email\").dropDuplicates([\"email\"])\n",
    "\n",
    "    base = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"customer_first_name\").alias(\"first_name\"),\n",
    "            col(\"customer_last_name\").alias(\"last_name\"),\n",
    "            col(\"customer_age\").alias(\"age\"),\n",
    "            col(\"customer_email\").alias(\"email\"),\n",
    "            col(\"customer_country\").alias(\"country_name\"),\n",
    "            col(\"customer_postal_code\").alias(\"postal_code\"),\n",
    "            col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "        )\n",
    "        .where(col(\"email\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    countries = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_countries\", properties=props\n",
    "    )\n",
    "    pets = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_pets\", properties=props\n",
    "    )\n",
    "\n",
    "    enriched = (\n",
    "        base\n",
    "        .join(countries, base.country_name == countries.country_name, \"left\")\n",
    "        .join(pets, base.pet_name == pets.pet_name, \"left\")\n",
    "        .select(\n",
    "            col(\"first_name\"),\n",
    "            col(\"last_name\"),\n",
    "            col(\"age\"),\n",
    "            col(\"email\"),\n",
    "            col(\"country_id\"),\n",
    "            col(\"postal_code\"),\n",
    "            col(\"pet_id\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_add = enriched.join(existing, [\"email\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"email\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_customers\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_customers\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_sellers_dimension(raw_df, url, props):\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_sellers\", properties=props\n",
    "    ).select(\"email\").dropDuplicates([\"email\"])\n",
    "\n",
    "    base = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"seller_first_name\").alias(\"first_name\"),\n",
    "            col(\"seller_last_name\").alias(\"last_name\"),\n",
    "            col(\"seller_email\").alias(\"email\"),\n",
    "            col(\"seller_country\").alias(\"country_name\"),\n",
    "            col(\"seller_postal_code\").alias(\"postal_code\"),\n",
    "        )\n",
    "        .where(col(\"email\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    countries = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_countries\", properties=props\n",
    "    )\n",
    "\n",
    "    enriched = (\n",
    "        base\n",
    "        .join(countries, base.country_name == countries.country_name, \"left\")\n",
    "        .select(\n",
    "            col(\"first_name\"),\n",
    "            col(\"last_name\"),\n",
    "            col(\"email\"),\n",
    "            col(\"country_id\"),\n",
    "            col(\"postal_code\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_add = enriched.join(existing, [\"email\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"email\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_sellers\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_sellers\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_stores_dimension(raw_df, url, props):\n",
    "    existing = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_stores\", properties=props\n",
    "    ).select(\"store_name\", \"location\").dropDuplicates([\"store_name\", \"location\"])\n",
    "\n",
    "    base = (\n",
    "        raw_df\n",
    "        .select(\n",
    "            col(\"store_name\"),\n",
    "            col(\"store_location\").alias(\"location\"),\n",
    "            col(\"store_city\").alias(\"city_name\"),\n",
    "            col(\"store_state\").alias(\"state\"),\n",
    "            col(\"store_country\").alias(\"country_name\"),\n",
    "            col(\"store_phone\").alias(\"phone\"),\n",
    "            col(\"store_email\").alias(\"email\")\n",
    "        )\n",
    "        .where(col(\"store_name\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    cities = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_cities\", properties=props\n",
    "    )\n",
    "    countries = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_countries\", properties=props\n",
    "    )\n",
    "\n",
    "    enriched = (\n",
    "        base\n",
    "        .join(cities, base.city_name == cities.city_name, \"left\")\n",
    "        .join(countries, base.country_name == countries.country_name, \"left\")\n",
    "        .select(\n",
    "            col(\"store_name\"),\n",
    "            col(\"location\"),\n",
    "            col(\"city_id\"),\n",
    "            col(\"state\"),\n",
    "            col(\"country_id\"),\n",
    "            col(\"phone\"),\n",
    "            col(\"email\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_add = enriched.join(existing, [\"store_name\", \"location\"], \"left_anti\") \\\n",
    "                     .dropDuplicates([\"store_name\", \"location\"])\n",
    "    add_count = to_add.count()\n",
    "    print(f\"Будет вставлено {add_count} новых записей в dim_stores\")\n",
    "\n",
    "    if add_count > 0:\n",
    "        to_add.write.jdbc(url=url, table=\"dim_stores\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def refresh_fact_sales(raw_df, url, props):\n",
    "    dates = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_dates\", properties=props\n",
    "    ).select(\"full_date\", \"date_id\").dropDuplicates([\"full_date\"])\n",
    "\n",
    "    customers = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_customers\", properties=props\n",
    "    ).select(\"email\", \"customer_id\").dropDuplicates([\"email\"])\n",
    "\n",
    "    sellers = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_sellers\", properties=props\n",
    "    ).select(\"email\", \"seller_id\").dropDuplicates([\"email\"])\n",
    "\n",
    "    products = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_products\", properties=props\n",
    "    ).select(\"product_name\", \"product_id\").dropDuplicates([\"product_name\"])\n",
    "\n",
    "    stores = raw_df.sparkSession.read.jdbc(\n",
    "        url=url, table=\"dim_stores\", properties=props\n",
    "    ).select(\"store_name\", \"store_id\").dropDuplicates([\"store_name\"])\n",
    "\n",
    "    facts_base = (\n",
    "        raw_df\n",
    "        .withColumn(\"sale_date_dt\", to_date(\"sale_date\", \"MM/dd/yyyy\"))\n",
    "        .select(\n",
    "            col(\"sale_date_dt\").alias(\"full_date\"),\n",
    "            col(\"customer_email\"),\n",
    "            col(\"seller_email\"),\n",
    "            col(\"product_name\"),\n",
    "            col(\"store_name\"),\n",
    "            col(\"sale_quantity\").alias(\"quantity\"),\n",
    "            col(\"sale_total_price\").alias(\"total_price\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    joined = (\n",
    "        facts_base\n",
    "        .join(dates, facts_base.full_date == dates.full_date, \"inner\")\n",
    "        .join(customers, facts_base.customer_email == customers.email, \"inner\")\n",
    "        .join(sellers, facts_base.seller_email == sellers.email, \"inner\")\n",
    "        .join(products, facts_base.product_name == products.product_name, \"inner\")\n",
    "        .join(stores, facts_base.store_name == stores.store_name, \"inner\")\n",
    "        .select(\n",
    "            col(\"date_id\"),\n",
    "            col(\"customer_id\"),\n",
    "            col(\"seller_id\"),\n",
    "            col(\"product_id\"),\n",
    "            col(\"store_id\"),\n",
    "            col(\"quantity\"),\n",
    "            col(\"total_price\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_facts = joined.count()\n",
    "    print(f\"Будет вставлено {total_facts} фактов в fact_sales\")\n",
    "\n",
    "    if total_facts > 0:\n",
    "        joined.write.jdbc(url=url, table=\"fact_sales\", mode=\"append\", properties=props)\n",
    "\n",
    "\n",
    "def initialize_clickhouse_session():\n",
    "    session = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"Spark_ETL_Reports_ClickHouse\")\n",
    "        .config(\"spark.jars\", \"postgresql-42.6.0.jar,clickhouse-jdbc-0.4.6.jar\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return session\n",
    "\n",
    "\n",
    "def main():\n",
    "    postgres_url = \"jdbc:postgresql://postgres:5432/spark_db\"\n",
    "    postgres_props = {\n",
    "        \"user\": \"spark_user\",\n",
    "        \"password\": \"spark_password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    pg_session = initialize_postgres_session()\n",
    "    source_df = load_raw_data(pg_session, postgres_url, postgres_props)\n",
    "\n",
    "    refresh_countries_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_cities_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_dates_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_simple_lookup(source_df, postgres_url, postgres_props)\n",
    "    refresh_pets_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_suppliers_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_product_dimensions(source_df, postgres_url, postgres_props)\n",
    "    refresh_customers_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_sellers_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_stores_dimension(source_df, postgres_url, postgres_props)\n",
    "    refresh_fact_sales(source_df, postgres_url, postgres_props)\n",
    "\n",
    "    pg_session.stop()\n",
    "\n",
    "    ch_session = initialize_clickhouse_session()\n",
    "    ch_session.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d019774-70c5-4783-aded-102877bedcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10_products: 3 rows\n",
      "revenue_by_category: 3 rows\n",
      "rating_reviews: 3 rows\n",
      "top10_customers: 10 rows\n",
      "customers_by_country: 204 rows\n",
      "avg_check_per_customer: 10000 rows\n",
      "monthly_trends: 12 rows\n",
      "yearly_revenue: 1 rows\n",
      "avg_order_size_by_month: 12 rows\n",
      "top5_stores: 5 rows\n",
      "sales_by_city_country: 383 rows\n",
      "avg_check_per_store: 383 rows\n",
      "top5_suppliers: 3 rows\n",
      "avg_price_by_supplier: 3 rows\n",
      "sales_by_supplier_country: 2 rows\n",
      "highest_rating: 1 rows\n",
      "lowest_rating: 1 rows\n",
      "most_reviewed: 3 rows\n",
      "rating↔units_sold correlation: 0.0069\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "def create_spark_session():\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"ETL_Reports_ClickHouse\")\n",
    "        .config(\"spark.jars\", \"postgresql-42.6.0.jar,clickhouse-jdbc-0.4.6.jar\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "def load_dimensions(spark, pg_url, pg_props):\n",
    "    fact_sales    = spark.read.jdbc(url=pg_url, table=\"fact_sales\",    properties=pg_props)\n",
    "    dim_products  = spark.read.jdbc(url=pg_url, table=\"dim_products\",  properties=pg_props)\n",
    "    dim_customers = spark.read.jdbc(url=pg_url, table=\"dim_customers\", properties=pg_props)\n",
    "    dim_dates     = spark.read.jdbc(url=pg_url, table=\"dim_dates\",     properties=pg_props)\n",
    "    dim_stores    = spark.read.jdbc(url=pg_url, table=\"dim_stores\",    properties=pg_props)\n",
    "    dim_suppliers = spark.read.jdbc(url=pg_url, table=\"dim_suppliers\", properties=pg_props)\n",
    "    dim_countries = spark.read.jdbc(url=pg_url, table=\"dim_countries\", properties=pg_props)\n",
    "    dim_cities    = spark.read.jdbc(url=pg_url, table=\"dim_cities\",    properties=pg_props)\n",
    "    return {\n",
    "        \"fact_sales\":    fact_sales,\n",
    "        \"dim_products\":  dim_products,\n",
    "        \"dim_customers\": dim_customers,\n",
    "        \"dim_dates\":     dim_dates,\n",
    "        \"dim_stores\":    dim_stores,\n",
    "        \"dim_suppliers\": dim_suppliers,\n",
    "        \"dim_countries\": dim_countries,\n",
    "        \"dim_cities\":    dim_cities,\n",
    "    }\n",
    "\n",
    "def write_to_clickhouse(df, table_name, order_cols, ch_url, ch_props):\n",
    "    df.write.format(\"jdbc\") \\\n",
    "       .option(\"url\", ch_url) \\\n",
    "       .option(\"dbtable\", table_name) \\\n",
    "       .options(**{\n",
    "           **ch_props,\n",
    "           \"createTableOptions\": f\"ENGINE = MergeTree() ORDER BY ({','.join(order_cols)})\"\n",
    "       }) \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .save()\n",
    "    print(f\"{table_name}: {df.count()} rows\")\n",
    "\n",
    "def sales_by_product(dimensions, ch_url, ch_props):\n",
    "    fact_sales   = dimensions[\"fact_sales\"]\n",
    "    dim_products = dimensions[\"dim_products\"]\n",
    "\n",
    "    prod_metrics = (\n",
    "        fact_sales\n",
    "          .join(dim_products, \"product_id\")\n",
    "          .select(\"product_id\", \"product_name\", \"category_id\", \"quantity\", \"total_price\", \"rating\", \"reviews\")\n",
    "    )\n",
    "\n",
    "    top10_products = (\n",
    "        prod_metrics\n",
    "          .groupBy(\"product_id\", \"product_name\")\n",
    "          .agg(F.sum(\"quantity\").alias(\"units_sold\"))\n",
    "          .orderBy(F.col(\"units_sold\").desc())\n",
    "          .limit(10)\n",
    "    )\n",
    "\n",
    "    revenue_by_category = (\n",
    "        prod_metrics\n",
    "          .groupBy(\"category_id\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "    )\n",
    "\n",
    "    rating_reviews = (\n",
    "        prod_metrics\n",
    "          .groupBy(\"product_id\", \"product_name\")\n",
    "          .agg(\n",
    "             F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "             F.sum(\"reviews\").alias(\"total_reviews\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(top10_products,      \"top10_products\",      [\"units_sold\"],      ch_url, ch_props)\n",
    "    write_to_clickhouse(revenue_by_category, \"revenue_by_category\", [\"category_id\"],     ch_url, ch_props)\n",
    "    write_to_clickhouse(rating_reviews,      \"rating_reviews\",      [\"product_id\"],      ch_url, ch_props)\n",
    "\n",
    "def sales_by_customer(dimensions, ch_url, ch_props):\n",
    "    fact_sales    = dimensions[\"fact_sales\"]\n",
    "    dim_customers = dimensions[\"dim_customers\"]\n",
    "    dim_countries = dimensions[\"dim_countries\"]\n",
    "\n",
    "    top10_customers = (\n",
    "        fact_sales\n",
    "          .join(dim_customers, \"customer_id\")\n",
    "          .groupBy(\"customer_id\", \"first_name\", \"last_name\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"total_spent\"))\n",
    "          .orderBy(F.col(\"total_spent\").desc())\n",
    "          .limit(10)\n",
    "    )\n",
    "\n",
    "    customers_by_country = (\n",
    "        fact_sales\n",
    "          .join(dim_customers, \"customer_id\")\n",
    "          .join(dim_countries, [\"country_id\"], \"left\")\n",
    "          .groupBy(\"country_name\")\n",
    "          .agg(F.countDistinct(\"customer_id\").alias(\"unique_customers\"))\n",
    "    )\n",
    "\n",
    "    avg_check_per_customer = (\n",
    "        fact_sales\n",
    "          .groupBy(\"customer_id\")\n",
    "          .agg((F.sum(\"total_price\") / F.count(\"*\")).alias(\"avg_check\"))\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(top10_customers,        \"top10_customers\",        [\"total_spent\"],      ch_url, ch_props)\n",
    "    write_to_clickhouse(customers_by_country,   \"customers_by_country\",   [\"country_name\"],     ch_url, ch_props)\n",
    "    write_to_clickhouse(avg_check_per_customer, \"avg_check_per_customer\", [\"customer_id\"],      ch_url, ch_props)\n",
    "\n",
    "def sales_by_time(dimensions, ch_url, ch_props):\n",
    "    fact_sales = dimensions[\"fact_sales\"]\n",
    "    dim_dates   = dimensions[\"dim_dates\"].withColumnRenamed(\"full_date\", \"sale_date\")\n",
    "\n",
    "    monthly_trends = (\n",
    "        fact_sales\n",
    "          .join(dim_dates, \"date_id\")\n",
    "          .withColumn(\"year\",  F.year(\"sale_date\"))\n",
    "          .withColumn(\"month\", F.month(\"sale_date\"))\n",
    "          .groupBy(\"year\", \"month\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "    )\n",
    "\n",
    "    yearly_revenue = (\n",
    "        monthly_trends\n",
    "          .groupBy(\"year\")\n",
    "          .agg(F.sum(\"revenue\").alias(\"yearly_revenue\"))\n",
    "    )\n",
    "\n",
    "    avg_order_size_by_month = (\n",
    "        fact_sales\n",
    "          .join(dim_dates, \"date_id\")\n",
    "          .withColumn(\"year\",  F.year(\"sale_date\"))\n",
    "          .withColumn(\"month\", F.month(\"sale_date\"))\n",
    "          .groupBy(\"year\", \"month\")\n",
    "          .agg((F.sum(\"total_price\") / F.count(\"*\")).alias(\"avg_order_size\"))\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(monthly_trends,          \"monthly_trends\",          [\"year\", \"month\"], ch_url, ch_props)\n",
    "    write_to_clickhouse(yearly_revenue,          \"yearly_revenue\",          [\"year\"],          ch_url, ch_props)\n",
    "    write_to_clickhouse(avg_order_size_by_month, \"avg_order_size_by_month\", [\"year\", \"month\"], ch_url, ch_props)\n",
    "\n",
    "def sales_by_store(dimensions, ch_url, ch_props):\n",
    "    fact_sales = dimensions[\"fact_sales\"]\n",
    "    dim_stores  = dimensions[\"dim_stores\"]\n",
    "    dim_cities  = dimensions[\"dim_cities\"]\n",
    "    dim_countries = dimensions[\"dim_countries\"]\n",
    "\n",
    "    top5_stores = (\n",
    "        fact_sales\n",
    "          .join(dim_stores, \"store_id\")\n",
    "          .groupBy(\"store_id\", \"store_name\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "          .orderBy(F.col(\"revenue\").desc())\n",
    "          .limit(5)\n",
    "    )\n",
    "\n",
    "    sales_by_city_country = (\n",
    "        fact_sales\n",
    "          .join(dim_stores, \"store_id\")\n",
    "          .join(dim_cities,    [\"city_id\"],    \"left\")\n",
    "          .join(dim_countries, [\"country_id\"], \"left\")\n",
    "          .groupBy(\"city_name\", \"country_name\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "    )\n",
    "\n",
    "    avg_check_per_store = (\n",
    "        fact_sales\n",
    "          .groupBy(\"store_id\")\n",
    "          .agg((F.sum(\"total_price\") / F.count(\"*\")).alias(\"avg_check\"))\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(top5_stores,           \"top5_stores\",           [\"revenue\"],         ch_url, ch_props)\n",
    "    write_to_clickhouse(sales_by_city_country, \"sales_by_city_country\", [\"city_name\"],       ch_url, ch_props)\n",
    "    write_to_clickhouse(avg_check_per_store,   \"avg_check_per_store\",   [\"store_id\"],        ch_url, ch_props)\n",
    "\n",
    "def sales_by_supplier(dimensions, ch_url, ch_props):\n",
    "    fact_sales    = dimensions[\"fact_sales\"]\n",
    "    dim_products  = dimensions[\"dim_products\"]\n",
    "    dim_suppliers = dimensions[\"dim_suppliers\"]\n",
    "    dim_countries = dimensions[\"dim_countries\"]\n",
    "\n",
    "    sales_with_supp = fact_sales.join(dim_products.select(\"product_id\", \"supplier_id\"), \"product_id\")\n",
    "\n",
    "    top5_suppliers = (\n",
    "        sales_with_supp\n",
    "          .groupBy(\"supplier_id\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "          .orderBy(F.col(\"revenue\").desc())\n",
    "          .limit(5)\n",
    "    )\n",
    "\n",
    "    avg_price_by_supplier = (\n",
    "        sales_with_supp\n",
    "          .groupBy(\"supplier_id\")\n",
    "          .agg((F.sum(\"total_price\") / F.sum(\"quantity\")).alias(\"avg_price\"))\n",
    "    )\n",
    "\n",
    "    sales_by_supplier_country = (\n",
    "        sales_with_supp\n",
    "          .join(dim_suppliers,  \"supplier_id\")\n",
    "          .join(dim_countries, [\"country_id\"], \"left\")\n",
    "          .groupBy(\"country_name\")\n",
    "          .agg(F.sum(\"total_price\").alias(\"revenue\"))\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(top5_suppliers,            \"top5_suppliers\",            [\"revenue\"],    ch_url, ch_props)\n",
    "    write_to_clickhouse(avg_price_by_supplier,     \"avg_price_by_supplier\",     [\"supplier_id\"],ch_url, ch_props)\n",
    "    write_to_clickhouse(sales_by_supplier_country, \"sales_by_supplier_country\", [\"country_name\"],ch_url, ch_props)\n",
    "\n",
    "def product_quality(dimensions, ch_url, ch_props):\n",
    "    fact_sales   = dimensions[\"fact_sales\"]\n",
    "    dim_products = dimensions[\"dim_products\"]\n",
    "\n",
    "    quality = (\n",
    "        fact_sales\n",
    "          .join(dim_products, \"product_id\")\n",
    "          .select(\"product_id\", \"product_name\", \"rating\", \"reviews\", \"quantity\")\n",
    "    )\n",
    "\n",
    "    window_desc = Window.orderBy(F.col(\"rating\").desc())\n",
    "    window_asc  = Window.orderBy(F.col(\"rating\").asc())\n",
    "\n",
    "    highest_rating = (\n",
    "        quality\n",
    "          .withColumn(\"rn\", row_number().over(window_desc))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    lowest_rating = (\n",
    "        quality\n",
    "          .withColumn(\"rn\", row_number().over(window_asc))\n",
    "          .filter(F.col(\"rn\") == 1)\n",
    "          .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    corr_val = quality.stat.corr(\"rating\", \"quantity\")\n",
    "\n",
    "    most_reviewed = (\n",
    "        quality\n",
    "          .groupBy(\"product_id\", \"product_name\")\n",
    "          .agg(F.sum(\"reviews\").alias(\"total_reviews\"))\n",
    "          .orderBy(F.col(\"total_reviews\").desc())\n",
    "          .limit(10)\n",
    "    )\n",
    "\n",
    "    write_to_clickhouse(highest_rating, \"highest_rating\", [\"rating\"],        ch_url, ch_props)\n",
    "    write_to_clickhouse(lowest_rating,  \"lowest_rating\",  [\"rating\"],        ch_url, ch_props)\n",
    "    write_to_clickhouse(most_reviewed,  \"most_reviewed\",  [\"total_reviews\"], ch_url, ch_props)\n",
    "\n",
    "    print(f\"rating↔units_sold correlation: {corr_val:.4f}\")\n",
    "\n",
    "\n",
    "spark = create_spark_session()\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/spark_db\"\n",
    "pg_props = {\n",
    "    \"user\": \"spark_user\",\n",
    "    \"password\": \"spark_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "ch_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_props = {\n",
    "    \"driver\":   \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "    \"user\":     \"custom_user\",\n",
    "    \"password\": \"custom_password\",\n",
    "}\n",
    "dims = load_dimensions(spark, pg_url, pg_props)\n",
    "sales_by_product(dims, ch_url, ch_props)\n",
    "sales_by_customer(dims, ch_url, ch_props)\n",
    "sales_by_time(dims, ch_url, ch_props)\n",
    "sales_by_store(dims, ch_url, ch_props)\n",
    "sales_by_supplier(dims, ch_url, ch_props)\n",
    "product_quality(dims, ch_url, ch_props)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17119dc-5f5e-427a-a2df-312e6f2952b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
